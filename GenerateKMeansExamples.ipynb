{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "852919c8-c4ba-4115-99d1-3ee1cc78d413",
   "metadata": {},
   "source": [
    "### Generate k-means datasets for practice\n",
    "### This tool can also dirty up the data, so it generates datasets that are good for practicing clean-up exercises\n",
    "#### Note the 2 kinds of dirty data: devious and not so devious.\n",
    "\n",
    "### Bill Nicholson\n",
    "### nicholdw@ucmail.uc.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88d0276c-fdda-4ffa-9846-f380f9a9846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "from collections import OrderedDict\n",
    "import colorsys\n",
    "from random import randrange\n",
    "from datetime import timedelta\n",
    "from datetime import *\n",
    "from random import normalvariate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "354a70b2-ba5a-4956-adf8-324a08194b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(msg, fileName, mode):\n",
    "    # mode: 'w' = write, 'a+' = append\n",
    "    with open('fileName.txt', 'w') as f:\n",
    "        print('Filename:', filename, file=f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "642e9d6b-5bcb-414f-92f6-bbcf5ca6238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/470690/how-to-automatically-generate-n-distinct-colors/30881059#30881059\n",
    "def get_colors(num_colors):\n",
    "    colors=[]\n",
    "    for i in np.arange(0., 360., 360. / num_colors):\n",
    "        hue = i/360.\n",
    "        lightness = (50 + np.random.rand() * 10)/100.\n",
    "        saturation = (90 + np.random.rand() * 10)/100.\n",
    "        colors.append(colorsys.hls_to_rgb(hue, lightness, saturation))\n",
    "        #print(\"get_colors(\", num_colors, \"): Computed colors:\", colors)\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad50dc2e-d199-4450-b88d-fa9cf2823b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeEuclideanDistance(p1, p2):\n",
    "    euclideanDistance = sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)\n",
    "    return euclideanDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe54a4dd-5fda-4bf5-b11e-270ea8726a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def targetClusterToString(targetCluster):\n",
    "    toString = \"\"\n",
    "    delimiter = \"\"\n",
    "    for key in targetCluster.keys():\n",
    "        toString += delimiter + key + \":\" + str(targetCluster[key])\n",
    "        delimiter = \"-\"\n",
    "    return \"(\" + toString + \")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "136f5c72-1c33-4525-b964-a386c2b201a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDeviousDirtyData():\n",
    "    return [0, None, 1, np.NaN, \" \", \"Missing\", \"None\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8149ca5-c920-4d72-b8b4-482f081e7639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirtyUpTheData(dataPoint, beDevious = True):\n",
    "    '''\n",
    "    Change a data value to some dirty value\n",
    "    :param dataPoint the value to dirty up\n",
    "    :beDevious whether or not to use dirty values than can't be commonly cleaned with dropna: \"Missing\" in quotes, \"None\" in quotes, etc.\n",
    "    '''\n",
    "    if random.randint(0,100) > 95:\n",
    "        #howMessedUp = random.randint(0,3)\n",
    "        # There's some devious values in here that dropna will not catch!\n",
    "        if beDevious:\n",
    "             # Remember the None constant in a numeric column is a float and will come back from the CSV file as NaN\n",
    "            tmp = random.choice(getDeviousDirtyData()) \n",
    "        else:\n",
    "            tmp = random.choice([None, np.NaN,])\n",
    "    else:\n",
    "        tmp = dataPoint\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13368efa-5b3c-4009-b241-6fad28064650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomDate(startDateString, endDateString):\n",
    "    \"\"\"\n",
    "    This function will return a random datetime between two datetime objects.\n",
    "    \"\"\"\n",
    "    startDate = datetime.strptime(startDateString, '%Y-%m-%d')\n",
    "    endDate = datetime.strptime(endDateString, '%Y-%m-%d')\n",
    "    deltaDays = endDate - startDate\n",
    "    #print(deltaDays, type(deltaDays))\n",
    "    randomDays = randrange(deltaDays.days)\n",
    "    return (startDate + timedelta(days = randomDays)).date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eff1f2b-6680-4312-8c69-ab6c8ff2d969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/35472461/select-one-element-from-a-list-using-python-following-the-normal-distribution\n",
    "def normal_choice(lst, mean=None, stddev=None):\n",
    "    '''\n",
    "    Pick an item from a list using a normal distribution\n",
    "    This is more fun than randomly picking, which results in a uniform distribution\n",
    "    '''\n",
    "    if mean is None:\n",
    "        # if mean is not specified, use center of list\n",
    "        mean = (len(lst) - 1) / 2\n",
    "\n",
    "    if stddev is None:\n",
    "        # if stddev is not specified, let list be -3 .. +3 standard deviations\n",
    "        stddev = len(lst) / 6\n",
    "\n",
    "    while True:\n",
    "        index = int(normalvariate(mean, stddev) + 0.5)\n",
    "        if 0 <= index < len(lst):\n",
    "            return lst[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0162a02d-00d1-4874-b256-3538d24ef7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDataFrame(myDF, columnsToConsider = None):\n",
    "    '''\n",
    "    Take out the yucky data\n",
    "    https://www.w3schools.com/python/pandas/ref_df_dropna.asp\n",
    "    '''\n",
    "    if columnsToConsider != None:\n",
    "        myDF.dropna(subset = columnsToConsider, inplace = True)\n",
    "    else:\n",
    "        myDF.dropna(inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9aea0ab-bf92-4622-bd04-d810637770f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(dirName):\n",
    "    if not os.path.exists(dirName):\n",
    "        os.makedirs(dirName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c23e0742-0b64-4877-a2fb-ee249387b123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processADataset(file):\n",
    "    targetCentroidColumnName = \"Target Centroid\"\n",
    "    path = \"./\"\n",
    "    mode = \"w\"\n",
    "    mySeed = 42                # Default\n",
    "    myDistribution = \"normal\"  # Default\n",
    "    logFile = None\n",
    "    logFileName = None\n",
    "    mySigma = 100            # Wild guess\n",
    "    makeItDirty = False\n",
    "    #print(\"seed =\", file[\"seed\"])\n",
    "    if file.get(\"seed\") != None: mySeed = int(file[\"seed\"])\n",
    "    if file.get(\"sigma\") != None: mySigma = int(file[\"sigma\"])\n",
    "    if file.get(\"totalpoints\") != None: numberOfRows = int(file[\"totalpoints\"])\n",
    "    if file.get(\"makeitdirty\") != None: \n",
    "        tmp = file[\"makeitdirty\"]\n",
    "        if tmp.lower() in ['true', '1', 't', 'y', 'yes', 'yeah', 'yup', 'certainly', 'uh-huh', 'ok', '-1']:\n",
    "            makeItDirty = True\n",
    "        else:\n",
    "            makeItDirty = False\n",
    "    # Gaussian doesn't work so don't use it\n",
    "    #if file.get(\"distribution\") != None: myDistribution = file[\"distribution\"]\n",
    "    random.seed(mySeed)\n",
    "    columnsToCluster = OrderedDict()\n",
    "    optimalClusters = int(file[\"optimal clusters\"])\n",
    "    targetClusters = []  # Will be populated as a list of dictionaries. Each sub dictionary is a point on the graph (x,y)\n",
    "    fileName = file[\"fileName\"]\n",
    "    print(\"-----------------\", fileName, \"------------------\")\n",
    "    mkdir(fileName)\n",
    "    path = \"./\" + fileName + \"/\"\n",
    "    logFileHandle = open(path + fileName + '.txt', 'w') \n",
    "    print(\"Creating\", fileName + \".csv\", \"optimal clusters = \", optimalClusters, file = logFileHandle)\n",
    "    print(\"makeitdirty =\", makeItDirty, file = logFileHandle)\n",
    "    print(\"seed =\", mySeed, file = logFileHandle)\n",
    "    print(\"distribution =\", myDistribution, file = logFileHandle)\n",
    "    print(\"total points =\", numberOfRows, file = logFileHandle)\n",
    "    if myDistribution == \"gaussian\": print(\"sigma =\", mySigma, file = logFileHandle)\n",
    "    headerRow = [str(x) for x in file[\"columns\"].keys()]\n",
    "    print(\"Header row = \", headerRow, file = logFileHandle)\n",
    "    # We can nest with/open statements! cool!\n",
    "    with open(path + fileName + \" with solution\" + \".csv\", mode) as DatasetWithSolution:\n",
    "        with open(path + fileName + \".csv\", mode) as Dataset:\n",
    "            if mode == \"w\":\n",
    "                DatasetWithSolution.write(\",\".join(headerRow) + \",\" + targetCentroidColumnName); DatasetWithSolution.write(\"\\n\")\n",
    "                Dataset.write(\",\".join(headerRow)); Dataset.write(\"\\n\")\n",
    "            for column in file[\"columns\"]:\n",
    "                if column in file[\"columns to cluster\"]:\n",
    "                    columnsToCluster[column] = file[\"columns\"][column]\n",
    "            print(\"Columns to cluster:\", columnsToCluster, file = logFileHandle)\n",
    "            if columnsToCluster == []:\n",
    "                raise Exception(\"No columns to cluster were found in the meta data.\")\n",
    "            # Compute the centroids around which the clusters will form\n",
    "            for i in range(0, int(optimalClusters)):\n",
    "                # we need a randomly generated (x, y) for the cluster.\n",
    "                # x and y are the two columns we are operating on as specified in \"columns to cluster\" in the meta data file\n",
    "                # for that we need the max/min for x and y\n",
    "                point = OrderedDict()\n",
    "\n",
    "                for key in columnsToCluster.keys():\n",
    "                    max = int(columnsToCluster[key][\"max\"])\n",
    "                    min = int(columnsToCluster[key][\"min\"])\n",
    "                    dataType = \"int\"\n",
    "                    decimalPlaces = 2\n",
    "                    if columnsToCluster[key].get(\"datatype\") != None:\n",
    "                        dataType = columnsToCluster[key][\"datatype\"]\n",
    "                    if columnsToCluster[key].get(\"decimalplaces\") != None:\n",
    "                        dataType = columnsToCluster[key][\"decimalplaces\"]\n",
    "                    # We need some random value in the range (max,min) that is sufficiently far enough away list the other clusters we are generating here\n",
    "                    # Nudge the points away from the edges\n",
    "                    featureRange = int(max) - int(min)\n",
    "                    featureMiddle = int(min) + (featureRange / 2)\n",
    "                    #point[key] = featureMiddle + int(random.uniform(-.3, .3) * featureRange)\n",
    "                    lower = int(int(min) + (featureRange * .23))\n",
    "                    upper = int(int(max) - (featureRange * .23))\n",
    "                    #print(\"key\", key, \"min\", min, \"max\", max, \"upper\", upper, \"lower\", lower)\n",
    "                    point[key] = random.randint(lower, upper)\n",
    "                    #print(\"key =\", key, \"max = \", max, \"min =\", min, \"featureRange =\", featureRange, \"featureMiddle =\", featureMiddle)\n",
    "                    #print(\"upper =\", upper, \"lower =\", lower, \"co-ordinate =\", point[key]) \n",
    "                #print(\"target cluster:\", point)\n",
    "                targetClusters.append(point)\n",
    "\n",
    "            print(\"Target clusters computed to be:\", file = logFileHandle)\n",
    "            for targetCluster in targetClusters:\n",
    "                print(targetCluster, file = logFileHandle)\n",
    "            for rowNumber in range(0, int(numberOfRows/optimalClusters)):\n",
    "                for i in range(0, len(targetClusters)):\n",
    "                    row = OrderedDict()  # The row of data we will write to the CSV file at the bottom of this loop\n",
    "                    for columnName in file[\"columns\"]:\n",
    "                        # column is a dictionary\n",
    "                        #print(column, \":\", file[\"columns\"][column])\n",
    "                        column = file[\"columns\"][columnName]\n",
    "                        if chooseFromList := column.get(\"choosefromlist\") != None:    # Walrus operator!\n",
    "                            #row[columnName] = random.choice(column[\"choosefromlist\"])\n",
    "                            row[columnName] = normal_choice(column[\"choosefromlist\"])\n",
    "                        else:\n",
    "                            # Do not convert from string, yet. We don't know what data type we are working with. Could be a date.\n",
    "                            max = column[\"max\"]\n",
    "                            min = column[\"min\"]\n",
    "                            dataType = \"int\"\n",
    "                            #print(\"Looking for datatype in \", column)\n",
    "                            if column.get(\"datatype\") != None:\n",
    "                                dataType = column[\"datatype\"]\n",
    "                                #print(\"Found datatype in\", column, \", set to\", dataType)\n",
    "                            decimalPlaces = 2\n",
    "                            if column.get(\"decimalplaces\") != None:\n",
    "                                decimalPlaces = column[\"decimalplaces\"]\n",
    "                            increment = column[\"increment\"]\n",
    "                            if dataType == \"float\":\n",
    "                                value = round(random.uniform(int(min), int(max)), decimalPlaces)\n",
    "                                #print(\"found float data type in \", column, \"generated value =\", value)\n",
    "                            elif dataType == \"int\":\n",
    "                                value = random.randint(int(min), int(max))\n",
    "                            elif dataType == \"date\":\n",
    "                                value = getRandomDate(str(min), str(max))\n",
    "                            elif dataType == \"time\":\n",
    "                                pass\n",
    "                            else:\n",
    "                                print(\"Invalid Data Type (\", datatype, \")\")\n",
    "                            row[columnName] = value   # Don't make it a string! \n",
    "                    # Now we have a randomly generated row but we do not know if it's near any of our centroid points\n",
    "                    #print(\"before:\", row)\n",
    "                    newValues = dict()\n",
    "                    randomTargetCluster = random.choice(targetClusters)\n",
    "                    row[\"Target Cluster\"] = targetClusterToString(randomTargetCluster)\n",
    "                    for column in columnsToCluster.keys():\n",
    "                        #x = row[column]\n",
    "                        # Get the vaules of the centroid, then we will nudge the values in our current row to be inthe neighborhood of that point\n",
    "                        newValues[column] = randomTargetCluster[column]\n",
    "                    for key in newValues.keys():\n",
    "                        if myDistribution == \"normal\":\n",
    "                            increment = (newValues[key] * random.uniform(-.2, .2))\n",
    "                            # At this time the only data types for cluster dimensions are int and float\n",
    "                            #print(\"row[\",key,\"]:\", row[key], type(row[key]), \", \",  isinstance(row[key], int))\n",
    "                            if isinstance(row[key], int):\n",
    "                                row[key] = int(newValues[key] + increment)\n",
    "                            else:\n",
    "                                # ToDo Need to round here according to decimalplaces value in the metadata, not hard-coded to 2\n",
    "                                row[key] = round(float(newValues[key] + increment), 2)\n",
    "                        elif myDistribution == \"gaussian\":\n",
    "                            mu = 5  #randomTargetCluster[key]\n",
    "                            row[key] = newValues[key] + (int(random.gauss(mu, sigma)))                           \n",
    "                        if random.random() > .90:\n",
    "                            randomness = int(row[key] * random.random()) * random.randint(-1, 1)\n",
    "                            tmp = row[key] + randomness\n",
    "                            #print(\"row =\", row, \"\\n  key =\", key, \"row[key] =\", row[key], \"randomness =\", randomness, tmp =\", tmp)\n",
    "                            if tmp >= int(file[\"columns\"][key][\"min\"]) and tmp <= int(file[\"columns\"][key][\"max\"]):\n",
    "                                row[key] = tmp\n",
    "                                #print(\"Adding \", increment, \"to get\", row[key] + tmp)\n",
    "                    #print(\"after:\", row)\n",
    "                    rowString = \"\"\n",
    "                    rowStringWithSolution = \"\"\n",
    "                    comma = \"\"\n",
    "                    #print(row)\n",
    "                    #break\n",
    "                    for key in row.keys():\n",
    "                        if makeItDirty and key != \"Target Cluster\":\n",
    "                            tmp = dirtyUpTheData(row[key])\n",
    "                        else:\n",
    "                            tmp = row[key]\n",
    "                        if key == \"Target Cluster\":\n",
    "                            rowStringWithSolution += comma + str(tmp)\n",
    "                        else:\n",
    "                            rowString += comma + str(tmp)\n",
    "                            rowStringWithSolution += comma + str(tmp)\n",
    "                            \n",
    "                        comma = \",\"\n",
    "                    DatasetWithSolution.write(rowStringWithSolution); DatasetWithSolution.write(\"\\n\")\n",
    "                    Dataset.write(rowString); Dataset.write(\"\\n\")\n",
    "    \n",
    "    logFileHandle.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ea5b89c-781a-4768-9e11-ed650bbd89b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDatasets(practiceDatasets):\n",
    "    fileCount = 0\n",
    "    for file in practiceDatasets[\"files\"]:\n",
    "        processADataset(file)\n",
    "        fileCount = fileCount + 1\n",
    "    print(fileCount, \" datasets processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be7ffc8-3536-4d85-b2fb-72c58766f13f",
   "metadata": {},
   "source": [
    "### Test one of the data files we generated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d900d69-e5f1-46f9-8978-4087e7b2b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testOurGeneratedDataFile(fileName, columnsToFit, cleanFirst = False):\n",
    "    fileCOunt = 0\n",
    "    myDF = pd.read_csv(fileName + \".csv\")\n",
    "    if cleanFirst:\n",
    "        cleanDataFrame(myDF, columnsToFit)\n",
    "    # drop all the columns except the two we are fitting to\n",
    "    myDF = myDF[columnsToFit]\n",
    "    logFileHandle = open(fileName + '.txt', 'a+') \n",
    "    #print(myDF.head())\n",
    "    inertias = []\n",
    "    myRange = range(1,11)\n",
    "    for i in myRange:\n",
    "        kmeans = KMeans(n_clusters=i)\n",
    "        kmeans.fit(myDF)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "\n",
    "    plt.plot(myRange, inertias, marker='o')\n",
    "    plt.title('Elbow method')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.rcParams['figure.figsize'] = [18, 18]  # I don't know why this works but it does seem to pleasingly resize the subplots\n",
    "    plt.show()\n",
    "    \n",
    "    # Scatter plot of the DataFrame overlaid with the scatter plot of the centroids (K-means)\n",
    "    # We will guess there are three clusters: that should match the \"optimal clusters\" value in the meta data file\n",
    "    kmeans = KMeans(n_clusters = 3)\n",
    "    #print(myDF.describe())\n",
    "    kmeans.fit(myDF)\n",
    "\n",
    "    # The labels_ property is an array of integers indicating the centroid to which each data point has been assigned.\n",
    "    #print(\"kmeans.labels_ =\", kmeans.labels_)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_ylabel(\"Property Tax\")\n",
    "    ax.set_xlabel(\"Square Feet\")\n",
    "    plt.scatter(myDF['square feet'], myDF['annual property tax'], c=kmeans.labels_)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    #print(\"Data type of centroid data:\", type(centroids))\n",
    "    print(\"Actual shape of centroid data:\", centroids.shape, file = logFileHandle)\n",
    "    print(\"Actual centroids computed by fit method:\", file = logFileHandle)\n",
    "    for centroid in centroids:\n",
    "        print(type(centroid), centroid, file = logFileHandle)\n",
    "    plt.scatter(centroids[:,0] , centroids[:,1] , s = 160, c=get_colors(len(centroids))) # color = myColors)\n",
    "    plt.show()\n",
    "\n",
    "    # Save a a copy of our definitive centroids for possible classification later.\n",
    "    happyCentroids = kmeans.cluster_centers_    \n",
    "    logFileHandle.close()\n",
    "\n",
    "    # Plot all the points in the same color just to get a feel for the points without classificaiton\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_ylabel(\"Property Tax\")\n",
    "    ax.set_xlabel(\"Square Feet\")\n",
    "    plt.scatter(myDF['square feet'], myDF['annual property tax'])\n",
    "    plt.show()\n",
    "\n",
    "    # Would be interesting: plot points in each classifications in a different color using the original centroids in the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f52889f-c3a4-41ae-a3c0-296fefe74882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploreACSVFileWithDirtyData(fileName, cleanFirst = False):\n",
    "    myDF = pd.read_csv(fileName + \".csv\")\n",
    "    print(myDF.info())\n",
    "    print(myDF.describe())\n",
    "    print(myDF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8140952-8da1-42d3-ac08-3be5e73e6ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- houses ------------------\n",
      "----------------- cars ------------------\n",
      "----------------- carsDirtyData ------------------\n",
      "----------------- dishCollection ------------------\n",
      "----------------- carpetInventory ------------------\n",
      "----------------- constructionProjects ------------------\n",
      "----------------- vacations ------------------\n",
      "----------------- hikes ------------------\n",
      "----------------- manufacturing history ------------------\n",
      "----------------- CarSales ------------------\n",
      "----------------- PowerToolCollection ------------------\n",
      "----------------- BankDeposits ------------------\n",
      "----------------- KielLockCommercialTraffic ------------------\n",
      "----------------- Brownville Food Pantry For Deer ------------------\n",
      "----------------- Popular Search Phrases by Date ------------------\n",
      "----------------- Produce Deliveries ------------------\n",
      "16  datasets processed.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    fileHandle = open(\"PracticeDatasetsMetaData.json\")\n",
    "    practiceDatasets = json.load(fileHandle)\n",
    "    #print(practiceDatasets)\n",
    "    generateDatasets(practiceDatasets)\n",
    "    #testOurGeneratedDataFile(\"./houses/houses\", [\"square feet\", \"annual property tax\"])\n",
    "    #testOurGeneratedDataFile(\"./PowerToolCollection/PowerToolCollection\", [\"cost\", \"hours of use\"], cleanFirst = True)\n",
    "    #testOurGeneratedDataFile(\"./carsales/carsales\", [\"total miles\", \"sale price\"])\n",
    "    #exploreACSVFileWithDirtyData(\"./carsDirtyData/carsDirtyData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ba45470-9721-43f7-bc1d-8bc25a055f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Experimental code follows..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd56041a-8109-4aa2-85a5-fd67a753306c",
   "metadata": {},
   "outputs": [],
   "source": [
    " #testOurGeneratedDataFile(\"PowerToolCollection\", [\"cost\", \"hours of use\"], cleanFirst = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d79fa5b5-a7d7-499e-988e-41f94e0eb4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment():\n",
    "    fileName = \"test\"\n",
    "    myDF = pd.DataFrame([\n",
    "        [\"a\", 2, 3],\n",
    "        [\"4\", None, 6],\n",
    "        [None, 7, 8],\n",
    "        [\"9\", 10, 11]\n",
    "    ])\n",
    "    print(myDF)\n",
    "    print(myDF.info())\n",
    "    #myDF.dropna(inplace=True)\n",
    "    #print(myDF)\n",
    "    #print(myDF.info())\n",
    "    myDF.to_csv(fileName + \".csv\")\n",
    "    newDF = pd.read_csv(fileName + \".csv\")\n",
    "    print(newDF)\n",
    "    print(newDF.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a20357-ff60-4e64-b158-d30fdd14f0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8a9f35f-7bb6-400e-9562-19285d05555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc6916ab-5389-42ad-b3d0-69b729e6f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"file1.txt\", \"w\") as f1:\n",
    "    with open(\"file2.txt\", \"w\") as f2:\n",
    "        for x in [1,2,3,4,5,\"EEE\", 1.25]:\n",
    "            f1.write(str(x))\n",
    "            f2.write(str(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffe1fe5-173b-4914-a446-686be03e88e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
